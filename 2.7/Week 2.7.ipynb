{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import joblib \n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('sensor.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Split the data into training and testing and validation sets\n",
    "train_df = df[df['timestamp'] < '2018-07-01']\n",
    "valid_df = df[(df['timestamp'] >= '2018-07-01') & (df['timestamp'] < '2018-08-01')]\n",
    "test_df = df[df['timestamp'] >= '2018-08-01'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these three parts to separate CSV files:\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "valid_df.to_csv('valid.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this is anomaly detection on time-series data, one possible method is to use an Isolation Forest. This is an unsupervised learning algorithm that works well for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "df_train = pd.read_csv('train.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Inspect the data\n",
    "print(df_train.isna().sum())\n",
    "print(df_train.var())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "# Dropping the columns we won't use\n",
    "df_train.drop(['Unnamed: 0', 'timestamp', 'machine_status'], axis=1, inplace=True)\n",
    "\n",
    "# Fill any NaN values with the mean\n",
    "df_train.fillna(df_train.mean(), inplace=True)\n",
    "\n",
    "# Drop 'sensor_15' column\n",
    "df_train.drop('sensor_15', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, check again if any NaN values still exist\n",
    "print(df_train.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_train), columns=df_train.columns)\n",
    "\n",
    "# Define the model\n",
    "model = IsolationForest(contamination=0.05)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(df_scaled)\n",
    "\n",
    "# Apply the trained model to the data\n",
    "scores = model.decision_function(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and the scaler\n",
    "joblib.dump(model, 'model.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileProcessor(FileSystemEventHandler):\n",
    "    '''This class manages new data files from the input directory.\n",
    "\n",
    "    It reacts to new file creation events by loading the file, transforming its data, making predictions with\n",
    "    a pre-trained model, and saving the results.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, configuration):\n",
    "        '''Initializes the FileProcessor with the provided configuration.\n",
    "\n",
    "        Args:\n",
    "            configuration (dict): Configuration dictionary containing paths to model, scaler, logs and other parameters.\n",
    "        '''\n",
    "\n",
    "        self.configuration = configuration\n",
    "        # Loading the pre-trained model from the file\n",
    "        self.loaded_model = joblib.load(configuration['model_path'])\n",
    "        # Loading the data scaler from the file\n",
    "        self.data_scaler = joblib.load(configuration['scaler_path'])\n",
    "        # Initializing logging\n",
    "        self.initialize_logging()\n",
    "\n",
    "    def initialize_logging(self):\n",
    "        '''Establishes logging to write to the log file as defined in the configuration.'''\n",
    "        logging.basicConfig(filename=self.configuration['log_path'], level=logging.INFO)\n",
    "        logging.info('Application initiated.')\n",
    "\n",
    "    def on_created(self, event):\n",
    "        '''This method is invoked when a new file is created in the input directory.\n",
    "\n",
    "        It loads the data, processes it, makes predictions, saves the results and logs the process.\n",
    "\n",
    "        Args:\n",
    "            event (FileSystemEvent): Event representing file system changes.\n",
    "        '''\n",
    "\n",
    "        file_name = event.src_path\n",
    "        logging.info(f'Detected new file: {file_name}')\n",
    "\n",
    "        try:\n",
    "            # Loading the data from the new file\n",
    "            loaded_data = pd.read_csv(file_name)\n",
    "            # Processing the data for model input\n",
    "            processed_data = self.preprocess_data(loaded_data)\n",
    "            # Making predictions with the loaded model\n",
    "            model_predictions = self.loaded_model.predict(processed_data)\n",
    "            # Appending predictions to the processed data\n",
    "            updated_predictions = processed_data.copy()\n",
    "            updated_predictions['prediction'] = model_predictions\n",
    "            # Saving results in the output directory with the same filename\n",
    "            updated_predictions.to_csv(os.path.join(self.configuration['output_directory'], os.path.basename(file_name)))\n",
    "\n",
    "            # Visualizing sensor data if specified in the configuration\n",
    "            for sensor in self.configuration['sensors_to_draw']:\n",
    "                self.visualize_sensor(updated_predictions, sensor)\n",
    "\n",
    "            logging.info('File processing completed.')\n",
    "        except Exception as ex:\n",
    "            logging.error(f'Error while processing file: {ex}')\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        '''This method processes the data into a format acceptable by the model.\n",
    "\n",
    "        It fills the missing values with mean of the column and scales the data using the pre-loaded scaler.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame): Raw data to be processed.\n",
    "\n",
    "        Returns:\n",
    "            data (pandas.DataFrame): Processed data ready for model input.\n",
    "        '''\n",
    "\n",
    "        data = data.fillna(data.mean())\n",
    "        data = pd.DataFrame(self.data_scaler.transform(data), columns=data.columns)\n",
    "        return data\n",
    "\n",
    "    def visualize_sensor(self, data, sensor):\n",
    "        '''This method visualizes the sensor data and saves it as a file.\n",
    "\n",
    "        It plots the specified sensor's data and saves the plot in the image directory specified in the configuration.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame): Data containing the sensor values.\n",
    "            sensor (str): Sensor to be visualized.\n",
    "        '''\n",
    "\n",
    "        figure, axis = plt.subplots()\n",
    "        data[sensor].plot(ax=axis)\n",
    "        # Saving the plot to a file\n",
    "        figure.savefig(os.path.join(self.configuration['image_directory'], f'{sensor}.png'))\n",
    "\n",
    "\n",
    "def parse_config(config_file):\n",
    "    '''This function parses the configuration from the given JSON file.\n",
    "\n",
    "    Args:\n",
    "        config_file (str): Path to the JSON configuration file.\n",
    "\n",
    "    Returns:\n",
    "        configuration (dict): Configuration parameters as a dictionary.\n",
    "    '''\n",
    "\n",
    "    with open(config_file) as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def application():\n",
    "    '''Main application function.\n",
    "\n",
    "    It loads the configuration, initializes the FileProcessor and starts the file system observer to watch for new files.\n",
    "    It keeps running until an interrupt signal is received.\n",
    "    '''\n",
    "\n",
    "    # Loading configuration from JSON file\n",
    "    config_data = parse_config('config.json')\n",
    "    # Initializing the file processor with configuration\n",
    "    file_observer = FileProcessor(config_data)\n",
    "\n",
    "    # Setting up file event observer to watch the input directory\n",
    "    file_event_observer = Observer()\n",
    "    file_event_observer.schedule(file_observer, config_data['input_directory'], recursive=False)\n",
    "    file_event_observer.start()\n",
    "\n",
    "    # Keep the program running until an interrupt signal is received\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        file_event_observer.stop()\n",
    "\n",
    "    file_event_observer.join()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    application()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
